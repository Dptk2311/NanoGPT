
# ðŸ§  Tiny Stories with NanoGPT

Fine-tuning [NanoGPT](https://github.com/karpathy/nanoGPT) on the [TinyStories dataset](https://huggingface.co/datasets/roneneldan/TinyStories) to generate small, whimsical stories using a lightweight GPT architecture.

---

## ðŸš€ Project Overview

This project demonstrates:
- How to preprocess and train NanoGPT on a small, meaningful dataset.
- Insights from training loss curves.
- Sample outputs from inference to see story quality.
- Experiments with hyperparameters for better generation.

---

## ðŸ“‚ Dataset

- **TinyStories** is a dataset of AI-generated children's stories, small enough to train fast and meaningful enough to evaluate output quality.
- Downloaded via HuggingFace Datasets: `roneneldan/TinyStories`.

---

## ðŸ§ª Features

- Fine-tuning from scratch and continuing training.
- Loss visualization (train/test split).
- Text generation after training.
- Hyperparameter tuning playground.

---

## ðŸ““ Notebook

- [Another_copy_of_Tiny_Stories_with_NanoGPT.ipynb](Another_copy_of_Tiny_Stories_with_NanoGPT.ipynb)

---

## ðŸ“ˆ Sample Results

```text
Story: There was a boy named Timmy. Tim loved to play outside in the town. One day he saw something new in the kitchen. He wanted to see it all the house, but it looked funny. He tried to find his way back, but he tripped and fell to his knee. He felt bad. He didn't know what to do. The little boy started to cry and asked if he could help.
